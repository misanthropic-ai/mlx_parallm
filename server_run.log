INFO:__main__:Starting server with initial model: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:__main__:Server will listen on 127.0.0.1:8000
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
INFO:     Started server process [20280]
INFO:     Waiting for application startup.
INFO:root:Using model from environment: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:root:Attempting to load initial model from CLI: mlx-community/Llama-3.2-3B-Instruct-4bit
Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 86778.70it/s]
INFO:root:Successfully loaded model: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:root:Model mlx-community/Llama-3.2-3B-Instruct-4bit registered with status: loaded
INFO:root:Batch processing worker task created.
INFO:root:Streaming batch worker task created.
INFO:root:Batch processing worker starting. MAX_BATCH_SIZE=8, BATCH_TIMEOUT=0.1s
WARNING:root:Model 'None' not immediately available in registry for batch worker. Will retry.
INFO:root:Streaming batch worker starting.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:60525 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:60526 - "GET /v1/models HTTP/1.1" 200 OK
INFO:root:Inside create_completion for model mlx-community/Llama-3.2-3B-Instruct-4bit. Parsed stream flag: False, type: <class 'bool'>
INFO:root:Non-streaming completion request for model mlx-community/Llama-3.2-3B-Instruct-4bit. It will be queued for batch processing.
ERROR:root:Request timed out for model mlx-community/Llama-3.2-3B-Instruct-4bit after 60.0 seconds.
INFO:     127.0.0.1:60534 - "POST /v1/completions HTTP/1.1" 504 Gateway Timeout
INFO:     127.0.0.1:60663 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:60664 - "GET /v1/models HTTP/1.1" 200 OK
INFO:root:Inside create_completion for model mlx-community/Llama-3.2-3B-Instruct-4bit. Parsed stream flag: False, type: <class 'bool'>
INFO:root:Non-streaming completion request for model mlx-community/Llama-3.2-3B-Instruct-4bit. It will be queued for batch processing.
ERROR:root:Request timed out for model mlx-community/Llama-3.2-3B-Instruct-4bit after 60.0 seconds.
INFO:     127.0.0.1:60665 - "POST /v1/completions HTTP/1.1" 504 Gateway Timeout
INFO:     127.0.0.1:60793 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:60794 - "GET /v1/models HTTP/1.1" 200 OK
INFO:root:Inside create_completion for model mlx-community/Llama-3.2-3B-Instruct-4bit. Parsed stream flag: False, type: <class 'bool'>
INFO:root:Non-streaming completion request for model mlx-community/Llama-3.2-3B-Instruct-4bit. It will be queued for batch processing.
ERROR:root:Request timed out for model mlx-community/Llama-3.2-3B-Instruct-4bit after 60.0 seconds.
INFO:     127.0.0.1:60795 - "POST /v1/completions HTTP/1.1" 504 Gateway Timeout
/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
