{
  "fine_tune_type": "lora",
  "num_layers": 8,
  "lora_parameters": {
    "rank": 16,
    "scale": 10.0,
    "dropout": 0.05,
    "keys": [
      "self_attn.q_proj",
      "self_attn.v_proj",
      "self_attn.k_proj",
      "self_attn.o_proj"
    ]
  },
  "model_path": "./models/hermes-qwen3-14b-4bit",
  "format": "safetensors",
  "zero_initialized": true,
  "trainable_params": 64
}