INFO:mlx_parallm.cli:Starting server with initial model: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:mlx_parallm.cli:Server will listen on 127.0.0.1:8000
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
INFO:     Started server process [72368]
INFO:     Waiting for application startup.
INFO:root:Batch config: MAX_BATCH_SIZE=8, BATCH_TIMEOUT=0.08s, REQUEST_TIMEOUT_SECONDS=600.0
INFO:root:Streaming concurrency limit: MAX_CONCURRENT_STREAMS=4
INFO:root:Attempting to load initial model from CLI: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:root:Successfully loaded model: mlx-community/Llama-3.2-3B-Instruct-4bit
INFO:root:Model mlx-community/Llama-3.2-3B-Instruct-4bit registered with status: loaded
INFO:root:Batch processing worker task created.
INFO:root:Streaming batch worker task created.
INFO:root:Batch processing worker starting. MAX_BATCH_SIZE=8, BATCH_TIMEOUT=0.08s
INFO:root:Batch worker successfully retrieved model 'mlx-community/Llama-3.2-3B-Instruct-4bit'.
INFO:root:Streaming batch worker starting.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:58399 - "GET /health HTTP/1.1" 200 OK
INFO:root:Streaming request received for model mlx-community/Llama-3.2-3B-Instruct-4bit. Stream flag: True
INFO:     127.0.0.1:58400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [72368]
