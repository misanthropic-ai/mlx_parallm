/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/subprocess.py:1021: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
[95m============================================================[0m
[95mMLX ParaLLM RL Training Launcher[0m
[95m============================================================[0m
Model: ./models/hermes-qwen3-14b-4bit
Mode: Mock Client
Steps: 1
Batch Size: 2
Diverse Mode: Enabled
[95m============================================================[0m

[1mStarting trainer...[0m
  Command: uv run mlx_parallm_train --model-path ./models/hermes-qwen3-14b-4bit --steps 1 --batch-size 2 --port 8000 --checkpoint-dir checkpoints --checkpoint-interval 1 --save-every-step true --diverse-mode true
  Working dir: /Users/shannon/Workspace/artivus/mlx_parallm

[1mTraining in progress...[0m
Logs are being saved to: logs/20250821_162439
Press Ctrl+C to stop

[92m[TRAINER][0m INFO:root:mlx_parallm_train starting
[92m[TRAINER][0m INFO:root:Server thread launched; waiting for readiness...
[92m[TRAINER][0m INFO:     Started server process [9913]
[92m[TRAINER][0m INFO:     Waiting for application startup.
[92m[TRAINER][0m INFO:root:Batch config: MAX_BATCH_SIZE=8, BATCH_TIMEOUT=0.1s, REQUEST_TIMEOUT_SECONDS=86400.0
[92m[TRAINER][0m INFO:root:Streaming concurrency limit: MAX_CONCURRENT_STREAMS=4
[92m[TRAINER][0m INFO:root:Scheduler mode: default
[92m[TRAINER][0m INFO:root:Diverse mode: True
[92m[TRAINER][0m INFO:root:Attempting to load initial model from CLI: ./models/hermes-qwen3-14b-4bit
[92m[TRAINER][0m INFO:root:Successfully loaded model: ./models/hermes-qwen3-14b-4bit
[92m[TRAINER][0m INFO:root:Model ./models/hermes-qwen3-14b-4bit registered with status: loaded
[92m[TRAINER][0m INFO:root:Batch processing worker task created.
[92m[TRAINER][0m INFO:root:Streaming batch worker task created.
[92m[TRAINER][0m INFO:root:Batch processing worker starting. MAX_BATCH_SIZE=8, BATCH_TIMEOUT=0.1s
[92m[TRAINER][0m INFO:root:Batch worker successfully retrieved model './models/hermes-qwen3-14b-4bit'.
[92m[TRAINER][0m INFO:root:Streaming batch worker starting.
[92m[TRAINER][0m INFO:     Application startup complete.
[92m[TRAINER][0m INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
[92m[TRAINER][0m INFO:root:Injecting LoRA: num_layers=8, rank=16, keys=['self_attn.q_proj', 'self_attn.v_proj']
[92m[TRAINER][0m INFO:root:Zero-initializing lora_b matrices to preserve base model behavior...
[92m[TRAINER][0m [91mWARNING:root:LoRA auto-init skipped due to error: 'Model' object has no attribute 'named_parameters'[0m
[92m[TRAINER][0m INFO:root:MockAtropos: Fetching batch 1/2, requesting n=2 completions
[92m[TRAINER][0m INFO:root:Non-streaming chat completion request for model ./models/hermes-qwen3-14b-4bit. It will be queued for batch processing.
[92m[TRAINER][0m INFO:root:Processing batch of 1 requests.
[92m[TRAINER][0m INFO:root:[DEBUG] Added 0 zero-width spaces for diversity (choice 1/2)
[92m[TRAINER][0m INFO:root:[DEBUG] Added 1 zero-width spaces for diversity (choice 2/2)
[92m[TRAINER][0m INFO:root:Calling batch_generate_text_util with 2 expanded prompts.
[92m[TRAINER][0m INFO:root:Batch fill=25.0% (queue_depth=0)
[92m[TRAINER][0m INFO:root:[DEBUG] DIVERSE_MODE=True, any_n_gt1=True
[92m[TRAINER][0m INFO:root:Using tokenizer's model_max_length: 131072
[92m[TRAINER][0m [93mWARNING:root:Effective max_length 131072 exceeds safety cap of 65536. Capping to 65536.[0m
[92m[TRAINER][0m INFO:root:Final effective_max_length for tokenizer: 65536
[92m[TRAINER][0m INFO:root:[DEBUG] requested_n=2, choices_raw count=2
[92m[TRAINER][0m INFO:root:Assembling response: requested_n=2, aggregated_choices=2
[92m[TRAINER][0m INFO:     127.0.0.1:51819 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m[TRAINER][0m INFO:root:MockAtropos(prefill=think) Response 1/2 (len=2056):
[92m[TRAINER][0m <think>
[92m[TRAINER][0m -`"â€"â€™â€™â€¦â€”.ï¼‰ï¼Ÿï¼›ï¼šâ€˜â€œã€Šã€‹ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•ã€•...
[92m[TRAINER][0m INFO:root:MockAtropos(prefill=think) Response 2/2 (len=2055):
[92m[TRAINER][0m <think>
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m â€‹
[92m[TRAINER][0m ...
[92m[TRAINER][0m INFO:root:  Completion 1: No \boxed answer found -> score=0.0
[92m[TRAINER][0m INFO:root:  Completion 2: No \boxed answer found -> score=0.0
[92m[TRAINER][0m INFO:root:MockAtropos: Fetching batch 2/2, requesting n=2 completions
[92m[TRAINER][0m INFO:root:Non-streaming chat completion request for model ./models/hermes-qwen3-14b-4bit. It will be queued for batch processing.
[92m[TRAINER][0m INFO:root:Processing batch of 1 requests.
[92m[TRAINER][0m INFO:root:[DEBUG] Added 0 zero-width spaces for diversity (choice 1/2)
[92m[TRAINER][0m INFO:root:[DEBUG] Added 1 zero-width spaces for diversity (choice 2/2)
[92m[TRAINER][0m INFO:root:Calling batch_generate_text_util with 2 expanded prompts.
[92m[TRAINER][0m INFO:root:Batch fill=25.0% (queue_depth=0)
[92m[TRAINER][0m INFO:root:[DEBUG] DIVERSE_MODE=True, any_n_gt1=True
[92m[TRAINER][0m INFO:root:Using tokenizer's model_max_length: 131072
[92m[TRAINER][0m [93mWARNING:root:Effective max_length 131072 exceeds safety cap of 65536. Capping to 65536.[0m
[92m[TRAINER][0m INFO:root:Final effective_max_length for tokenizer: 65536
[92m[TRAINER][0m INFO:root:[DEBUG] requested_n=2, choices_raw count=2
[92m[TRAINER][0m INFO:root:Assembling response: requested_n=2, aggregated_choices=2
[92m[TRAINER][0m INFO:     127.0.0.1:51953 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m[TRAINER][0m INFO:root:MockAtropos(prefill=think) Response 1/2 (len=9979):
[92m[TRAINER][0m <think>
[92m[TRAINER][0m ((x1âˆ’x2)^2+(y1âˆ’y2)^2+(z1âˆ’z2)^2)ï¼âˆš((x1âˆ’x2)^2+(y1âˆ’y2)^2+(z1âˆ’z2)^2)
[92m[TRAINER][0m 
[92m[TRAINER][0m è¿™ä¸ªç­‰å¼æè¿°çš„æ˜¯ä¸‰ç»´ç©ºé—´ä¸­ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»å…¬å¼ã€‚ç»™å®šä¸¤ä¸ªç‚¹ P1(x1, y1, z1) å’Œ P2(x2, y2, z2)ï¼Œå®ƒä»¬ä¹‹é—´çš„è·ç¦»å¯ä»¥é€šè¿‡ä¸Šé¢çš„å…¬å¼è®¡ç®—å¾—å‡ºï¼š
[92m[TRAINER][0m 
[92m[TRAINER][0m \[ \text{è·ç¦»} = \sqrt{(x1 - x2)^2 + (y1 - y2)^2 + (z1 - z2)^2} \]
[92m[TRAINER][0m 
[92m[TRAINER][0m è¿™ä¸ªå…¬å¼æ˜¯æ¬§å‡ é‡Œå¾—å‡ ä½•ä¸­çš„åŸºæœ¬æ¦‚å¿µä¹‹ä¸€ï¼Œç”¨äºè®¡ç®—ä¸¤ç‚¹é—´çš„ç›´çº¿è·ç¦»ã€‚å®ƒåŸºäºæ¯•è¾¾å“¥æ‹‰æ–¯å®šç†ï¼Œå¹¶è¢«æ‰©å±•åˆ°äº†ä¸‰ç»´ç©ºé—´ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å…ˆåˆ†åˆ«è®¡ç®—æ¯ä¸ªåæ ‡è½´ä¸Šä¸¤ç‚¹é—´çš„å·®å€¼ï¼Œç„¶åå°†è¿™äº›å·®å€¼å¹³æ–¹åç›¸åŠ ï¼Œæœ€åå–å¹³æ–¹æ ¹å¾—åˆ°æœ€ç»ˆçš„è·ç¦»ã€‚
[92m[TRAINER][0m 
[92m[TRAINER][0m ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªç‚¹ä½äºåæ ‡ç³»çš„åŸç‚¹(0, 0, 0)ï¼Œå¦ä¸€ä¸ªç‚¹ä½äº(3, 4, 0)ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç‚¹ServiÃ§oä¹‹é—´çš„è·ç¦»å°±æ˜¯ï¼š
[92m[TRAINER][0m 
[92m[TRAINER][0m \[ \sqrt{(3-0)^2 + (4-0)^2 + (0-0)^2} = \sqrt{9 + 16 + 0} = \sqrt{25} = 5 \]
[92m[TRAINER][0m 
[92m[TRAINER][0m è¿™è¡¨ç¤ºä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿è·ç¦»ä¸º5ä¸ªå•ä½é•¿åº¦...
[92m[TRAINER][0m INFO:root:MockAtropos(prefill=think) Response 2/2 (len=7425):
[92m[TRAINER][0m <think>
[92m[TRAINER][0m P1: ã€ŠNatureã€‹ï¼šé¦–æ¬¡å®ç°åŸå­çº§åˆ†è¾¨ç‡çš„äººç±»çº¿ç²’ä½“æ ¸ç³–ä½“ç»“æ„æµ‹å®šã€‚çº¿ç²’ä½“æ ¸ç³–ä½“(mt-ribosome)æ˜¯çº¿ç²’ä½“å†…å‘æŒ¥è›‹ç™½è´¨ç¿»è¯‘åŠŸèƒ½çš„å…³é”®è£…ç½®ã€‚åœ¨ç›®å‰çš„ç”Ÿç‰©è¿›åŒ–å²ä¸Šï¼Œçº¿ç²’ä½“è¢«è®¤ä¸ºæ˜¯ç‹¬ç«‹äºç»†èƒæ ¸çš„ç»†èƒå™¨ï¼Œèµ·æºäºç»†èŒã€‚è¿™å¯¼è‡´äº†å®ƒä»¬çš„æ ¸ç³–ä½“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç±»ä¼¼äºç»†èŒæ ¸ç³–ä½“ã€‚å› æ­¤ï¼Œå®ƒä»¬é€šå¸¸è¢«ç§°ä¸ºâ€œmitoribosomesâ€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨å†·å†»ç”µå­æ˜¾å¾®é•œæŠ€æœ¯é¦–æ¬¡è§£æäº†äººç±»çº¿ç²’ä½“æ ¸ç³–ä½“çš„é«˜åˆ†è¾¨ç‡ç»“æ„ã€‚è¿™ä¸€æˆæœå°†æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£çº¿ç²’ä½“çš„åŠŸèƒ½åŠå…¶åœ¨ç–¾ç—…ä¸­çš„ä½œç”¨ï¼Œä¸ºå¼€å‘é’ˆå¯¹çº¿ç²’ä½“ç–¾ç—…çš„æ²»ç–—ç­–ç•¥æä¾›æ–°çš„æ€è·¯ã€‚
[92m[TRAINER][0m P2ï¼šã€ŠNatureã€‹ï¼šæ­ç¤ºäº†äººç±»å…ç–«ç³»ç»Ÿä¸­ä¸€ç§å…³é”®çš„è°ƒèŠ‚æœºåˆ¶ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå…ç–«ç»†èƒé€šè¿‡ç‰¹å®šçš„ä¿¡å·é€šè·¯æ¥è°ƒæ§è‡ªèº«çš„æ´»æ€§ï¼Œé˜²æ­¢è¿‡åº¦æ¿€æ´»å¯¼è‡´ç»„ç»‡æŸä¼¤ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶äººå‘˜å‘ç°äº†ä¸€ç§åä¸ºâ€œPD-1â€çš„è›‹ç™½è´¨åœ¨æŠ‘åˆ¶Tç»†èƒæ´»åŒ–æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚PD-1è›‹ç™½é€šè¿‡ä¸é…ä½“ç»“åˆï¼Œä¼ é€’æŠ‘åˆ¶ä¿¡å·ï¼Œä»è€Œé™åˆ¶Tç»†èƒçš„è¿‡åº¦ååº”ã€‚è¿™ä¸€å‘ç°å¯¹äºç†è§£è‡ªèº«å…ç–«æ€§ç–¾ç—…çš„å‘ç”Ÿæœºåˆ¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶å¯èƒ½ä¸ºå¼€å‘æ–°çš„å…ç–«æ²»ç–—ç­–ç•¥æä¾›ä¾æ®ã€‚
[92m[TRAINER][0m P3ï¼šã€ŠNatureã€‹ï¼šé¦–æ¬¡æŠ¥é“äº†åœ¨é‡å­è®¡ç®—é¢†åŸŸå–å¾—çš„é‡å¤§çªç ´ã€‚ç ”ç©¶äºº...
[92m[TRAINER][0m INFO:root:  Completion 1: No \boxed answer found -> score=0.0
[92m[TRAINER][0m INFO:root:  Completion 2: No \boxed answer found -> score=0.0
[92m[TRAINER][0m INFO:root:[train] step=1 metrics={'loss': 0.0, 'mean_logp': -0.14588418354893826, 'tokens': 8194, 'clip_ratio': 0.0, 'pos_clip_ratio': 0.0, 'neg_clip_ratio': 0.0, 'kl': 0.0}
[92m[TRAINER][0m INFO:root:Smoke training completed.

[92m============================================================[0m
[92mâœ“ Training completed successfully![0m
[92m============================================================[0m

[93mCleaning up processes...[0m
[92mAll processes terminated[0m
Logs saved to: logs/20250821_162439

[93mCleaning up processes...[0m
[92mAll processes terminated[0m
Logs saved to: logs/20250821_162439
